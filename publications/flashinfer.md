---
layout: publication
title: "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving"
authors: ["Zihao Ye", "Lequn Chen", "Ruihang Lai", "Wuwei Lin", "Yineng Zhang", "Stephanie Wang", "Tianqi Chen", "Baris Kasikci", "Vinod Grover", "Arvind Krishnamurthy", "Luis Ceze"]
venue: "Annual Conference on Machine Learning and Systems (MLSys)"
year: 2025
topics: ["LLM Serving"]
pdf: "https://arxiv.org/abs/2501.01005"
code: "https://github.com/flashinfer-ai/flashinfer"
link: "https://flashinfer.ai/2024/12/16/flashinfer-v02-release.html"
award: "Best Paper Award"
---
